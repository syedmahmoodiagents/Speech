{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUI0jHwlRvGNkPEPxJGsXL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedmahmoodiagents/Speech/blob/main/Speech_CNNvsRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edFmwem7ETOZ",
        "outputId": "bdd357c8-106c-4ceb-aeb1-fca7238413f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "FwmG9A7nDqkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa5E5JuEaqVx"
      },
      "outputs": [],
      "source": [
        "class SimpleASRDataset(Dataset):\n",
        "    def __init__(self, files, texts, vocab):\n",
        "        self.files = files\n",
        "        self.texts = texts\n",
        "        self.vocab = vocab\n",
        "        self.char_to_idx = {c:i+1 for i,c in enumerate(vocab)}  # 0=blank for CTC\n",
        "        self.idx_to_char = {i+1:c for i,c in enumerate(vocab)}\n",
        "\n",
        "        self.mfcc = torchaudio.transforms.MFCC(sample_rate=16000, n_mfcc=40)\n",
        "        self.max_len = 16000  # 1 sec clips for simplicity\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        return torch.tensor([self.char_to_idx[c] for c in text])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        audio, sr = torchaudio.load(path)\n",
        "\n",
        "        if audio.shape[0] > 1:\n",
        "            audio = audio.mean(dim=0, keepdim=True)\n",
        "        if sr != 16000:\n",
        "            audio = torchaudio.functional.resample(audio, sr, 16000)\n",
        "\n",
        "        if audio.shape[1] < self.max_len:\n",
        "            audio = F.pad(audio, (0, self.max_len - audio.shape[1]))\n",
        "        else:\n",
        "            audio = audio[:, :self.max_len]\n",
        "\n",
        "        mfcc = self.mfcc(audio).squeeze(0)   # [40, T]\n",
        "        mfcc = mfcc.transpose(0,1)           # [T, 40]\n",
        "\n",
        "        target = self.encode_text(text)\n",
        "\n",
        "        return mfcc, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    mfccs = [b[0] for b in batch]\n",
        "    targets = [b[1] for b in batch]\n",
        "\n",
        "    mfcc_lens = torch.tensor([x.shape[0] for x in mfccs])\n",
        "    tgt_lens   = torch.tensor([t.shape[0] for t in targets])\n",
        "\n",
        "    mfccs_padded = torch.nn.utils.rnn.pad_sequence(mfccs, batch_first=True)\n",
        "    targets_padded = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
        "\n",
        "    return mfccs_padded, mfcc_lens, targets_padded, tgt_lens"
      ],
      "metadata": {
        "id": "WUPMAiTLDuYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Convolution"
      ],
      "metadata": {
        "id": "yLjITFwXEEbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_ASR(nn.Module):\n",
        "    def __init__(self, n_mels=40, num_classes=30):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((1,2))  # reduce time dimension\n",
        "        )\n",
        "\n",
        "        # Corrected: n_mels dimension is halved after MaxPool2d\n",
        "        self.fc = nn.Linear(64*(n_mels // 2), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, 40]\n",
        "        x = x.unsqueeze(1)          # [B, 1, T, 40]\n",
        "        x = self.cnn(x)             # [B, 64, T_new, 20] (where 20 is n_mels // 2)\n",
        "        x = x.permute(0, 2, 1, 3)   # [B, T_new, 64, 20]\n",
        "        x = x.reshape(x.size(0), x.size(1), -1)  # flatten freq to [B, T_new, 64*20]\n",
        "        x = self.fc(x)              # [B, T_new, num_classes]\n",
        "        return x"
      ],
      "metadata": {
        "id": "bY8cIspDDNvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using RNN"
      ],
      "metadata": {
        "id": "G0VY-EpiEJ91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_ASR(nn.Module):\n",
        "    def __init__(self, n_mels=40, hidden=256, num_classes=30):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=n_mels, hidden_size=hidden, num_layers=3, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T, 40]\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out)    # [B, T, num_classes]\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "3ism8fNGDRXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "VxXem_KCEc_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "def train_model(model, name, loader, epochs=10):\n",
        "    model = model.to(device)\n",
        "    ctc = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for mfcc, mfcc_lens, tgt, tgt_lens in loader:\n",
        "            mfcc = mfcc.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            logits = model(mfcc)                 # [B, T, C]\n",
        "\n",
        "            # CTC expects shape [T, B, C]\n",
        "            logits = logits.transpose(0,1)\n",
        "            loss = ctc(logits,tgt,mfcc_lens,tgt_lens)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"{name} Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "pUdx7vE8DoOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"hello\", \"yes\", \"no\", \"open the door\"]\n",
        "\n",
        "# build character-level vocabulary\n",
        "vocab = sorted(list({c for t in texts for c in t}))\n",
        "# vocab = sorted([t for t in texts])\n",
        "print(\"Vocab:\", vocab)\n",
        "print(\"Vocab size:\", len(vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZrqUjRGFLH_",
        "outputId": "921b357f-3d50-4c45-e1b6-6650ce562958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab: [' ', 'd', 'e', 'h', 'l', 'n', 'o', 'p', 'r', 's', 't', 'y']\n",
            "Vocab size: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile"
      ],
      "metadata": {
        "id": "zjfaBLkKHXv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy .wav file for testing\n",
        "dummy_audio_path = \"dummy_audio.wav\"\n",
        "sample_rate = 16000\n",
        "duration = 1  # 1 second\n",
        "dummy_audio = np.random.uniform(low=-0.5, high=0.5, size=sample_rate * duration).astype(np.float32)\n",
        "##################################\n",
        "wavfile.write(dummy_audio_path, sample_rate, dummy_audio)"
      ],
      "metadata": {
        "id": "gnWusSUoHUDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [dummy_audio_path] * len(texts)"
      ],
      "metadata": {
        "id": "1aDzCMh-HcQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SimpleASRDataset(files, texts, vocab)\n",
        "loader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE9Gk-hxHyMc",
        "outputId": "5bc96003-b4f0-46d2-a516-43387cfebca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/functional/functional.py:582: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = CNN_ASR(num_classes=len(vocab)+1)\n",
        "rnn_model = RNN_ASR(num_classes=len(vocab)+1)"
      ],
      "metadata": {
        "id": "Z0kg3Lh9H-br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(cnn_model, \"CNN\", loader)\n",
        "print(\"-----------\")\n",
        "train_model(rnn_model, \"RNN\", loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQBNZr0sUJGp",
        "outputId": "de464fc9-9071-418b-e4b5-04abfddefd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Epoch 1: Loss = 44.6848\n",
            "CNN Epoch 2: Loss = -54.4574\n",
            "CNN Epoch 3: Loss = 13.2778\n",
            "CNN Epoch 4: Loss = 65.9578\n",
            "CNN Epoch 5: Loss = 93.1612\n",
            "CNN Epoch 6: Loss = 73.9377\n",
            "CNN Epoch 7: Loss = 37.0053\n",
            "CNN Epoch 8: Loss = -1.4626\n",
            "CNN Epoch 9: Loss = -12.8997\n",
            "CNN Epoch 10: Loss = -2.1957\n",
            "-----------\n",
            "RNN Epoch 1: Loss = -3.2533\n",
            "RNN Epoch 2: Loss = 11.7204\n",
            "RNN Epoch 3: Loss = 10.4251\n",
            "RNN Epoch 4: Loss = 4.9397\n",
            "RNN Epoch 5: Loss = -0.5747\n",
            "RNN Epoch 6: Loss = -2.7585\n",
            "RNN Epoch 7: Loss = -1.4666\n",
            "RNN Epoch 8: Loss = 1.5214\n",
            "RNN Epoch 9: Loss = 4.7736\n",
            "RNN Epoch 10: Loss = 6.6024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o-8NMRVRHCAh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}